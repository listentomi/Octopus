{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table actor:\n",
      "  Column first_name: first_name\n",
      "  Column last_name: last_name\n",
      "  Column last_update: last_update\n",
      "Table actor_info:\n",
      "  Column first_name: first_name\n",
      "  Column last_name: last_name\n",
      "Table address:\n",
      "  Column address: address\n",
      "  Column address2: address2\n",
      "  Column district: district\n",
      "  Column city_id: city_id\n",
      "  Column postal_code: postal_code\n",
      "  Column phone: phone\n",
      "  Column last_update: last_update\n",
      "Table category:\n",
      "  Column name: name\n",
      "  Column last_update: last_update\n",
      "Table city:\n",
      "  Column city: city\n",
      "  Column country_id: country_id\n",
      "  Column last_update: last_update\n",
      "Table country:\n",
      "  Column country: country\n",
      "  Column last_update: last_update\n",
      "Table customer:\n",
      "  Column store_id: store_id\n",
      "  Column first_name: first_name\n",
      "  Column last_name: last_name\n",
      "  Column email: email\n",
      "  Column address_id: address_id\n",
      "  Column active: active\n",
      "  Column create_date: create_date\n",
      "  Column last_update: last_update\n",
      "Table customer_list:\n",
      "  Column address: address\n",
      "  Column zip code: postal_code\n",
      "  Column phone: phone\n",
      "  Column city: city\n",
      "  Column country: country\n",
      "  Column SID: store_id\n",
      "Table film:\n",
      "  Column title: title\n",
      "  Column description: description\n",
      "  Column release_year: release_year\n",
      "  Column language_id: language_id\n",
      "  Column original_language_id: original_language_id\n",
      "  Column rental_duration: rental_duration\n",
      "  Column rental_rate: rental_rate\n",
      "  Column length: length\n",
      "  Column replacement_cost: replacement_cost\n",
      "  Column rating: rating\n",
      "  Column special_features: special_features\n",
      "  Column last_update: last_update\n",
      "Table film_actor:\n",
      "  Column last_update: last_update\n",
      "Table film_category:\n",
      "  Column last_update: last_update\n",
      "Table film_list:\n",
      "  Column title: title\n",
      "  Column description: description\n",
      "  Column category: name\n",
      "  Column price: rental_rate\n",
      "  Column length: length\n",
      "  Column rating: rating\n",
      "Table film_text:\n",
      "  Column title: title\n",
      "  Column description: description\n",
      "Table inventory:\n",
      "  Column film_id: film_id\n",
      "  Column store_id: store_id\n",
      "  Column last_update: last_update\n",
      "Table language:\n",
      "  Column name: name\n",
      "  Column last_update: last_update\n",
      "Table nicer_but_slower_film_list:\n",
      "  Column title: title\n",
      "  Column description: description\n",
      "  Column category: name\n",
      "  Column price: rental_rate\n",
      "  Column length: length\n",
      "  Column rating: rating\n",
      "Table payment:\n",
      "  Column customer_id: customer_id\n",
      "  Column staff_id: staff_id\n",
      "  Column rental_id: rental_id\n",
      "  Column amount: amount\n",
      "  Column payment_date: payment_date\n",
      "  Column last_update: last_update\n",
      "Table rental:\n",
      "  Column rental_date: rental_date\n",
      "  Column inventory_id: inventory_id\n",
      "  Column customer_id: customer_id\n",
      "  Column return_date: return_date\n",
      "  Column staff_id: staff_id\n",
      "  Column last_update: last_update\n",
      "Table sales_by_film_category:\n",
      "  Column category: name\n",
      "Table sales_by_store:\n",
      "Table staff:\n",
      "  Column first_name: first_name\n",
      "  Column last_name: last_name\n",
      "  Column address_id: address_id\n",
      "  Column picture: picture\n",
      "  Column email: email\n",
      "  Column store_id: store_id\n",
      "  Column active: active\n",
      "  Column username: username\n",
      "  Column password: password\n",
      "  Column last_update: last_update\n",
      "Table staff_list:\n",
      "  Column address: address\n",
      "  Column zip code: postal_code\n",
      "  Column phone: phone\n",
      "  Column city: city\n",
      "  Column country: country\n",
      "  Column SID: store_id\n",
      "Table store:\n",
      "  Column manager_staff_id: manager_staff_id\n",
      "  Column address_id: address_id\n",
      "  Column last_update: last_update\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "使用该方法将描述存到项目文件中，index by 数据库名\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_descriptions_from_database(cnx):\n",
    "    if not cnx:\n",
    "        return None\n",
    "    try:\n",
    "        cursor = cnx.cursor()\n",
    "       \n",
    "    \n",
    "        # 创建一个游标对象\n",
    "        cursor = cnx.cursor()\n",
    "        get_table = \"SHOW TABLES\"\n",
    "        cursor.execute(get_table)\n",
    "        tables = cursor.fetchall()\n",
    "        table_names = [table[0] for table in tables]\n",
    "        table_descriptions = {}\n",
    "        for table_name in table_names:\n",
    "            # 查询表结构信息\n",
    "            describe_query = f\"SHOW FULL COLUMNS FROM {table_name}\"\n",
    "            cursor.execute(describe_query)\n",
    "\n",
    "            # 获取字段描述信息\n",
    "            \n",
    "            column_descriptions = {}\n",
    "            \n",
    "            for column in cursor.fetchall():\n",
    "                column_name = column[0]\n",
    "                column_comment = column[8]\n",
    "                if column_comment:\n",
    "                    column_descriptions[column_name] = column_comment\n",
    "                \n",
    "            table_descriptions[table_name] = column_descriptions\n",
    "        return table_descriptions\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"to database err:{err}\")\n",
    "        return None\n",
    "\n",
    "    finally:\n",
    "        # 关闭游标和连接\n",
    "        cursor.close()\n",
    "        \n",
    "\n",
    "# 调用函数查询字段描述\n",
    "descriptions = get_descriptions_from_database(cnx)\n",
    "\n",
    "# 打印字段描述\n",
    "if descriptions:\n",
    "    for table, columns in descriptions.items():\n",
    "        print(f\"Table {table}:\")\n",
    "        for column, description in columns.items():\n",
    "            print(f\"  Column {column}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成datasource.json\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open('datasource.json', 'r') as f:\n",
    "    jsons = json.load(f)\n",
    "\n",
    "names = []\n",
    "for root, dirs, files in os.walk('./data/4_26/complex/'):\n",
    "    for file in files:\n",
    "        tt = file.split('.')[0].split('_')\n",
    "        tt = '_'.join(tt[:-1])\n",
    "        names.append(tt)\n",
    "\n",
    "for name in names:\n",
    "    if name not in [json['db_name'] for json in jsons]: \n",
    "        jsons.append({\n",
    "            \"db_name\": name,\n",
    "            \"url\": \"\",\n",
    "        })\n",
    "\n",
    "with open('datasource.json', 'w') as f:\n",
    "    json.dump(jsons, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get description for dbhome_credit_default in https://www.kaggle.com/c/home-credit-default-risk/data error ocur:list index out of range\n",
      "get description for dbprediction in https://www.kaggle.com/c/outbrain-click-prediction/data error ocur:list index out of range\n"
     ]
    }
   ],
   "source": [
    "# 获取kaggle中关于数据库的description,不是kaggle的手动添加到datasource.json中\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def get_descriptions_from_kaggle():\n",
    "    with open('datasource.json', 'r') as file:\n",
    "        urls = json.load(file)\n",
    "\n",
    "    for item in urls:\n",
    "        url = item['url']\n",
    "        if 'kaggle' not in url:\n",
    "            continue\n",
    "        db_name = item['db_name']\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        ld_json_tags = soup.find_all('script', type='application/ld+json')\n",
    "        try:\n",
    "            tags = json.loads(ld_json_tags[0].text.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"get description for db{db_name} in {url} error ocur:{e}\")\n",
    "            continue\n",
    "        descriptions = tags['description']\n",
    "        item['description'] = descriptions\n",
    "\n",
    "\n",
    "    with open('datasource.json', 'w') as file:\n",
    "        json.dump(urls, file, indent=4)\n",
    "\n",
    "get_descriptions_from_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 手动添加描述\n",
    "\n",
    "\n",
    "de = \"\"\"\n",
    "\n",
    "application_{train|test}.csv\n",
    "\n",
    "This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n",
    "Static data for all applications. One row represents one loan in our data sample.\n",
    "bureau.csv\n",
    "\n",
    "All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n",
    "For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n",
    "bureau_balance.csv\n",
    "\n",
    "Monthly balances of previous credits in Credit Bureau.\n",
    "This table has one row for each month of history of every previous credit reported to Credit Bureau – i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n",
    "POS_CASH_balance.csv\n",
    "\n",
    "Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n",
    "This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n",
    "credit_card_balance.csv\n",
    "\n",
    "Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n",
    "This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample – i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n",
    "previous_application.csv\n",
    "\n",
    "All previous applications for Home Credit loans of clients who have loans in our sample.\n",
    "There is one row for each previous application related to loans in our data sample.\n",
    "installments_payments.csv\n",
    "\n",
    "Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n",
    "There is a) one row for every payment that was made plus b) one row each for missed payment.\n",
    "One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\n",
    "HomeCredit_columns_description.csv\n",
    "\n",
    "This file contains descriptions for the columns in the various data files.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('datasource.json', 'r') as file:\n",
    "        urls = json.load(file)\n",
    "\n",
    "for item in urls:\n",
    "    if not item['db_name'] == 'home_credit_default':\n",
    "        continue\n",
    "    item['description'] = de\n",
    "\n",
    "with open('datasource.json', 'w') as file:\n",
    "        json.dump(urls, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: near \"order\": syntax error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Table categories contains columns: CategoryID(INTEGER), CategoryName(VARCHAR(15)), Description(TEXT), Picture(BLOB)',\n",
       " 'Table sqlite_sequence contains columns: name(), seq()',\n",
       " 'Table customercustomerdemo contains columns: CustomerID(VARCHAR(5)), CustomerTypeID(VARCHAR(10))',\n",
       " 'Table customerdemographics contains columns: CustomerTypeID(VARCHAR(10)), CustomerDesc(TEXT)',\n",
       " 'Table customers contains columns: CustomerID(VARCHAR(5)), CompanyName(VARCHAR(40)), ContactName(VARCHAR(30)), ContactTitle(VARCHAR(30)), Address(VARCHAR(60)), City(VARCHAR(15)), Region(VARCHAR(15)), PostalCode(VARCHAR(10)), Country(VARCHAR(15)), Phone(VARCHAR(24)), Fax(VARCHAR(24))',\n",
       " 'Table employees contains columns: EmployeeID(INTEGER), LastName(VARCHAR(20)), FirstName(VARCHAR(10)), Title(VARCHAR(30)), TitleOfCourtesy(VARCHAR(25)), BirthDate(DATETIME), HireDate(DATETIME), Address(VARCHAR(60)), City(VARCHAR(15)), Region(VARCHAR(15)), PostalCode(VARCHAR(10)), Country(VARCHAR(15)), HomePhone(VARCHAR(24)), Extension(VARCHAR(4)), Photo(BLOB), Notes(TEXT), ReportsTo(INTEGER), PhotoPath(VARCHAR(255)), Salary(FLOAT)',\n",
       " 'Table employeeterritories contains columns: EmployeeID(INTEGER), TerritoryID(VARCHAR(20))',\n",
       " 'Table orders contains columns: OrderID(INTEGER), CustomerID(VARCHAR(5)), EmployeeID(INTEGER), OrderDate(DATETIME), RequiredDate(DATETIME), ShippedDate(DATETIME), ShipVia(INTEGER), Freight(DECIMAL), ShipName(VARCHAR(40)), ShipAddress(VARCHAR(60)), ShipCity(VARCHAR(15)), ShipRegion(VARCHAR(15)), ShipPostalCode(VARCHAR(10)), ShipCountry(VARCHAR(15))',\n",
       " 'Table products contains columns: ProductID(INTEGER), ProductName(VARCHAR(40)), SupplierID(INTEGER), CategoryID(INTEGER), QuantityPerUnit(VARCHAR(20)), UnitPrice(DECIMAL), UnitsInStock(SMALLINT), UnitsOnOrder(SMALLINT), ReorderLevel(SMALLINT), Discontinued(TINYINT)',\n",
       " 'Table region contains columns: RegionID(INTEGER), RegionDescription(VARCHAR(50))',\n",
       " 'Table shippers contains columns: ShipperID(INTEGER), CompanyName(VARCHAR(40)), Phone(VARCHAR(24))',\n",
       " 'Table suppliers contains columns: SupplierID(INTEGER), CompanyName(VARCHAR(40)), ContactName(VARCHAR(30)), ContactTitle(VARCHAR(30)), Address(VARCHAR(60)), City(VARCHAR(15)), Region(VARCHAR(15)), PostalCode(VARCHAR(10)), Country(VARCHAR(15)), Phone(VARCHAR(24)), Fax(VARCHAR(24)), HomePage(TEXT)',\n",
       " 'Table territories contains columns: TerritoryID(VARCHAR(20)), TerritoryDescription(VARCHAR(50)), RegionID(INTEGER)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取sqlite元数据\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def extract_tables_and_columns(db_file):\n",
    "    conn = sqlite3.connect(db_file)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    tables_info = {}\n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        try:\n",
    "            cursor.execute(\"PRAGMA table_info({});\".format(table_name))\n",
    "        except sqlite3.DatabaseError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue \n",
    "        columns_info = cursor.fetchall()\n",
    "        columns = [(col[1], col[2]) for col in columns_info]\n",
    "        tables_info[table_name] = columns\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    structure_description = []\n",
    "    for table_name, columns_info in tables_info.items():\n",
    "        column_des = []\n",
    "        for column_name, column_type in columns_info:\n",
    "            des = f'{column_name}({column_type})'\n",
    "            column_des.append(des)\n",
    "        columns_info = ', '.join(column_des)\n",
    "        table_strucure = f'Table {table_name} contains columns: {columns_info}'\n",
    "        structure_description.append(table_strucure)\n",
    "\n",
    "    return tables_info, structure_description\n",
    "\n",
    "db_file = './database/northwind.sqlite'\n",
    "tables_info, structure_description = extract_tables_and_columns(db_file)\n",
    "\n",
    "structure_description \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to process airline\n",
      "database file ./database/airline.sqlite not exists, jump to next database\n",
      "database airport has no description, jump to next database\n",
      "{'db_name': 'airport', 'url': 'https://dev.mysql.com/doc/index-other.html'}\n",
      "database aspartic has no description, jump to next database\n",
      "{'db_name': 'aspartic', 'url': 'https://wikidbs.github.io/'}\n",
      "database athletics has no description, jump to next database\n",
      "{'db_name': 'athletics', 'url': 'https://wikidbs.github.io/'}\n",
      "database employees has no description, jump to next database\n",
      "{'db_name': 'employees', 'url': 'https://dev.mysql.com/doc/index-other.html'}\n",
      "start to process home_credit_default\n",
      "database home_credit_default description file already exists, jump to next database\n",
      "database hospital has no description, jump to next database\n",
      "{'db_name': 'hospital', 'url': 'https://wikidbs.github.io/'}\n",
      "start to process indicators_2022\n",
      "database indicators_2022 description file already exists, jump to next database\n",
      "database italy has no description, jump to next database\n",
      "{'db_name': 'italy', 'url': 'https://wikidbs.github.io/'}\n",
      "start to process matches_football\n",
      "database matches_football description file already exists, jump to next database\n",
      "database menagerie has no description, jump to next database\n",
      "{'db_name': 'menagerie', 'url': 'https://dev.mysql.com/doc/index-other.html'}\n",
      "start to process NBA\n",
      "database NBA description file already exists, jump to next database\n",
      "start to process pitchfork\n",
      "database pitchfork description file already exists, jump to next database\n",
      "start to process prediction\n",
      "database file ./database/prediction.sqlite not exists, jump to next database\n",
      "database production has no description, jump to next database\n",
      "{'db_name': 'production', 'url': ''}\n",
      "start to process race\n",
      "database race description file already exists, jump to next database\n",
      "database red_chamber has no description, jump to next database\n",
      "{'db_name': 'red_chamber', 'url': 'https://wikidbs.github.io/'}\n",
      "start to process sakila\n",
      "database sakila description file already exists, jump to next database\n",
      "start to process sales\n",
      "database file ./database/sales.sqlite not exists, jump to next database\n",
      "start to process spatiaLite_small\n",
      "Error: no such module: VirtualSpatialIndex\n",
      "Error: no such module: VirtualElementary\n",
      "start to process table spatial_ref_sys, indicators: ['spatial_ref_sys.srid', 'spatial_ref_sys.auth_name', 'spatial_ref_sys.auth_srid', 'spatial_ref_sys.ref_sys_name', 'spatial_ref_sys.proj4text', 'spatial_ref_sys.srtext']\n",
      "start to process table spatialite_history, indicators: ['spatialite_history.event_id', 'spatialite_history.table_name', 'spatialite_history.geometry_column', 'spatialite_history.event', 'spatialite_history.timestamp', 'spatialite_history.ver_sqlite', 'spatialite_history.ver_splite']\n",
      "start to process table sqlite_sequence, indicators: ['sqlite_sequence.name', 'sqlite_sequence.seq']\n",
      "start to process table geometry_columns, indicators: ['geometry_columns.f_table_name', 'geometry_columns.f_geometry_column', 'geometry_columns.geometry_type', 'geometry_columns.coord_dimension', 'geometry_columns.srid', 'geometry_columns.spatial_index_enabled']\n",
      "start to process table spatial_ref_sys_aux, indicators: ['spatial_ref_sys_aux.srid', 'spatial_ref_sys_aux.is_geographic', 'spatial_ref_sys_aux.has_flipped_axes', 'spatial_ref_sys_aux.spheroid', 'spatial_ref_sys_aux.prime_meridian', 'spatial_ref_sys_aux.datum', 'spatial_ref_sys_aux.projection', 'spatial_ref_sys_aux.unit', 'spatial_ref_sys_aux.axis_1_name', 'spatial_ref_sys_aux.axis_1_orientation', 'spatial_ref_sys_aux.axis_2_name', 'spatial_ref_sys_aux.axis_2_orientation']\n",
      "start to process table views_geometry_columns, indicators: ['views_geometry_columns.view_name', 'views_geometry_columns.view_geometry', 'views_geometry_columns.view_rowid', 'views_geometry_columns.f_table_name', 'views_geometry_columns.f_geometry_column', 'views_geometry_columns.read_only']\n",
      "start to process table virts_geometry_columns, indicators: ['virts_geometry_columns.virt_name', 'virts_geometry_columns.virt_geometry', 'virts_geometry_columns.geometry_type', 'virts_geometry_columns.coord_dimension', 'virts_geometry_columns.srid']\n",
      "start to process table geometry_columns_statistics, indicators: ['geometry_columns_statistics.f_table_name', 'geometry_columns_statistics.f_geometry_column', 'geometry_columns_statistics.last_verified', 'geometry_columns_statistics.row_count', 'geometry_columns_statistics.extent_min_x', 'geometry_columns_statistics.extent_min_y', 'geometry_columns_statistics.extent_max_x', 'geometry_columns_statistics.extent_max_y']\n",
      "start to process table views_geometry_columns_statistics, indicators: ['views_geometry_columns_statistics.view_name', 'views_geometry_columns_statistics.view_geometry', 'views_geometry_columns_statistics.last_verified', 'views_geometry_columns_statistics.row_count', 'views_geometry_columns_statistics.extent_min_x', 'views_geometry_columns_statistics.extent_min_y', 'views_geometry_columns_statistics.extent_max_x', 'views_geometry_columns_statistics.extent_max_y']\n",
      "start to process table virts_geometry_columns_statistics, indicators: ['virts_geometry_columns_statistics.virt_name', 'virts_geometry_columns_statistics.virt_geometry', 'virts_geometry_columns_statistics.last_verified', 'virts_geometry_columns_statistics.row_count', 'virts_geometry_columns_statistics.extent_min_x', 'virts_geometry_columns_statistics.extent_min_y', 'virts_geometry_columns_statistics.extent_max_x', 'virts_geometry_columns_statistics.extent_max_y']\n",
      "start to process table geometry_columns_field_infos, indicators: ['geometry_columns_field_infos.f_table_name', 'geometry_columns_field_infos.f_geometry_column', 'geometry_columns_field_infos.ordinal', 'geometry_columns_field_infos.column_name', 'geometry_columns_field_infos.null_values', 'geometry_columns_field_infos.integer_values', 'geometry_columns_field_infos.double_values', 'geometry_columns_field_infos.text_values', 'geometry_columns_field_infos.blob_values', 'geometry_columns_field_infos.max_size', 'geometry_columns_field_infos.integer_min', 'geometry_columns_field_infos.integer_max', 'geometry_columns_field_infos.double_min', 'geometry_columns_field_infos.double_max']\n",
      "start to process table views_geometry_columns_field_infos, indicators: ['views_geometry_columns_field_infos.view_name', 'views_geometry_columns_field_infos.view_geometry', 'views_geometry_columns_field_infos.ordinal', 'views_geometry_columns_field_infos.column_name', 'views_geometry_columns_field_infos.null_values', 'views_geometry_columns_field_infos.integer_values', 'views_geometry_columns_field_infos.double_values', 'views_geometry_columns_field_infos.text_values', 'views_geometry_columns_field_infos.blob_values', 'views_geometry_columns_field_infos.max_size', 'views_geometry_columns_field_infos.integer_min', 'views_geometry_columns_field_infos.integer_max', 'views_geometry_columns_field_infos.double_min', 'views_geometry_columns_field_infos.double_max']\n",
      "start to process table virts_geometry_columns_field_infos, indicators: ['virts_geometry_columns_field_infos.virt_name', 'virts_geometry_columns_field_infos.virt_geometry', 'virts_geometry_columns_field_infos.ordinal', 'virts_geometry_columns_field_infos.column_name', 'virts_geometry_columns_field_infos.null_values', 'virts_geometry_columns_field_infos.integer_values', 'virts_geometry_columns_field_infos.double_values', 'virts_geometry_columns_field_infos.text_values', 'virts_geometry_columns_field_infos.blob_values', 'virts_geometry_columns_field_infos.max_size', 'virts_geometry_columns_field_infos.integer_min', 'virts_geometry_columns_field_infos.integer_max', 'virts_geometry_columns_field_infos.double_min', 'virts_geometry_columns_field_infos.double_max']\n",
      "start to process table geometry_columns_time, indicators: ['geometry_columns_time.f_table_name', 'geometry_columns_time.f_geometry_column', 'geometry_columns_time.last_insert', 'geometry_columns_time.last_update', 'geometry_columns_time.last_delete']\n",
      "start to process table geometry_columns_auth, indicators: ['geometry_columns_auth.f_table_name', 'geometry_columns_auth.f_geometry_column', 'geometry_columns_auth.read_only', 'geometry_columns_auth.hidden']\n",
      "start to process table views_geometry_columns_auth, indicators: ['views_geometry_columns_auth.view_name', 'views_geometry_columns_auth.view_geometry', 'views_geometry_columns_auth.hidden']\n",
      "start to process table virts_geometry_columns_auth, indicators: ['virts_geometry_columns_auth.virt_name', 'virts_geometry_columns_auth.virt_geometry', 'virts_geometry_columns_auth.hidden']\n",
      "start to process table sql_statements_log, indicators: ['sql_statements_log.id', 'sql_statements_log.time_start', 'sql_statements_log.time_end', 'sql_statements_log.user_agent', 'sql_statements_log.sql_statement', 'sql_statements_log.success', 'sql_statements_log.error_cause']\n",
      "start to process table buildings_points, indicators: ['buildings_points.PK_UID', 'buildings_points.osm_id', 'buildings_points.name', 'buildings_points.type', 'buildings_points.Geometry']\n",
      "start to process table idx_buildings_points_Geometry_node, indicators: ['idx_buildings_points_Geometry_node.nodeno', 'idx_buildings_points_Geometry_node.data']\n",
      "start to process table idx_buildings_points_Geometry_rowid, indicators: ['idx_buildings_points_Geometry_rowid.rowid', 'idx_buildings_points_Geometry_rowid.nodeno']\n",
      "start to process table idx_buildings_points_Geometry_parent, indicators: ['idx_buildings_points_Geometry_parent.nodeno', 'idx_buildings_points_Geometry_parent.parentnode']\n",
      "start to process table landuse_polygon, indicators: ['landuse_polygon.PK_UID', 'landuse_polygon.osm_id', 'landuse_polygon.code', 'landuse_polygon.fclass', 'landuse_polygon.Geometry']\n",
      "start to process table idx_landuse_polygon_Geometry_node, indicators: ['idx_landuse_polygon_Geometry_node.nodeno', 'idx_landuse_polygon_Geometry_node.data']\n",
      "start to process table idx_landuse_polygon_Geometry_rowid, indicators: ['idx_landuse_polygon_Geometry_rowid.rowid', 'idx_landuse_polygon_Geometry_rowid.nodeno']\n",
      "start to process table idx_landuse_polygon_Geometry_parent, indicators: ['idx_landuse_polygon_Geometry_parent.nodeno', 'idx_landuse_polygon_Geometry_parent.parentnode']\n",
      "start to process table pois_polygon, indicators: ['pois_polygon.PK_UID', 'pois_polygon.osm_id', 'pois_polygon.code', 'pois_polygon.fclass', 'pois_polygon.name', 'pois_polygon.Geometry']\n",
      "start to process table idx_pois_polygon_Geometry_node, indicators: ['idx_pois_polygon_Geometry_node.nodeno', 'idx_pois_polygon_Geometry_node.data']\n",
      "start to process table idx_pois_polygon_Geometry_rowid, indicators: ['idx_pois_polygon_Geometry_rowid.rowid', 'idx_pois_polygon_Geometry_rowid.nodeno']\n",
      "start to process table idx_pois_polygon_Geometry_parent, indicators: ['idx_pois_polygon_Geometry_parent.nodeno', 'idx_pois_polygon_Geometry_parent.parentnode']\n",
      "start to process table roads_lines, indicators: ['roads_lines.PK_UID', 'roads_lines.osm_id', 'roads_lines.code', 'roads_lines.fclass', 'roads_lines.name', 'roads_lines.ref', 'roads_lines.oneway', 'roads_lines.maxspeed', 'roads_lines.layer', 'roads_lines.bridge', 'roads_lines.tunnel', 'roads_lines.Geometry']\n",
      "start to process table idx_roads_lines_Geometry_node, indicators: ['idx_roads_lines_Geometry_node.nodeno', 'idx_roads_lines_Geometry_node.data']\n",
      "start to process table idx_roads_lines_Geometry_rowid, indicators: ['idx_roads_lines_Geometry_rowid.rowid', 'idx_roads_lines_Geometry_rowid.nodeno']\n",
      "start to process table idx_roads_lines_Geometry_parent, indicators: ['idx_roads_lines_Geometry_parent.nodeno', 'idx_roads_lines_Geometry_parent.parentnode']\n",
      "start to process table waterways_lines, indicators: ['waterways_lines.PK_UID', 'waterways_lines.osm_id', 'waterways_lines.code', 'waterways_lines.fclass', 'waterways_lines.width', 'waterways_lines.name', 'waterways_lines.Geometry']\n",
      "start to process table idx_waterways_lines_Geometry_node, indicators: ['idx_waterways_lines_Geometry_node.nodeno', 'idx_waterways_lines_Geometry_node.data']\n",
      "start to process table idx_waterways_lines_Geometry_rowid, indicators: ['idx_waterways_lines_Geometry_rowid.rowid', 'idx_waterways_lines_Geometry_rowid.nodeno']\n",
      "start to process table idx_waterways_lines_Geometry_parent, indicators: ['idx_waterways_lines_Geometry_parent.nodeno', 'idx_waterways_lines_Geometry_parent.parentnode']\n",
      "start to process table sqlite_stat1, indicators: ['sqlite_stat1.tbl', 'sqlite_stat1.idx', 'sqlite_stat1.stat']\n",
      "start to process table sqlite_stat3, indicators: ['sqlite_stat3.tbl', 'sqlite_stat3.idx', 'sqlite_stat3.neq', 'sqlite_stat3.nlt', 'sqlite_stat3.ndlt', 'sqlite_stat3.sample']\n",
      "start to process table idx_buildings_points_Geometry, indicators: ['idx_buildings_points_Geometry.pkid', 'idx_buildings_points_Geometry.xmin', 'idx_buildings_points_Geometry.xmax', 'idx_buildings_points_Geometry.ymin', 'idx_buildings_points_Geometry.ymax']\n",
      "start to process table idx_landuse_polygon_Geometry, indicators: ['idx_landuse_polygon_Geometry.pkid', 'idx_landuse_polygon_Geometry.xmin', 'idx_landuse_polygon_Geometry.xmax', 'idx_landuse_polygon_Geometry.ymin', 'idx_landuse_polygon_Geometry.ymax']\n",
      "start to process table idx_pois_polygon_Geometry, indicators: ['idx_pois_polygon_Geometry.pkid', 'idx_pois_polygon_Geometry.xmin', 'idx_pois_polygon_Geometry.xmax', 'idx_pois_polygon_Geometry.ymin', 'idx_pois_polygon_Geometry.ymax']\n",
      "start to process table idx_roads_lines_Geometry, indicators: ['idx_roads_lines_Geometry.pkid', 'idx_roads_lines_Geometry.xmin', 'idx_roads_lines_Geometry.xmax', 'idx_roads_lines_Geometry.ymin', 'idx_roads_lines_Geometry.ymax']\n",
      "start to process table idx_waterways_lines_Geometry, indicators: ['idx_waterways_lines_Geometry.pkid', 'idx_waterways_lines_Geometry.xmin', 'idx_waterways_lines_Geometry.xmax', 'idx_waterways_lines_Geometry.ymin', 'idx_waterways_lines_Geometry.ymax']\n",
      "start to process spotify\n",
      "database file ./database/spotify.sqlite not exists, jump to next database\n",
      "start to process tennis\n",
      "database tennis description file already exists, jump to next database\n",
      "start to process tvmaze\n",
      "database tvmaze description file already exists, jump to next database\n",
      "start to process UK_arms\n",
      "database UK_arms description file already exists, jump to next database\n",
      "start to process WDI\n",
      "database WDI description file already exists, jump to next database\n",
      "start to process wildfire\n",
      "Error: no such module: VirtualSpatialIndex\n",
      "Error: no such module: VirtualElementary\n",
      "Error: no such module: VirtualKNN\n",
      "start to process table spatial_ref_sys, indicators: ['spatial_ref_sys.srid', 'spatial_ref_sys.auth_name', 'spatial_ref_sys.auth_srid', 'spatial_ref_sys.ref_sys_name', 'spatial_ref_sys.proj4text', 'spatial_ref_sys.srtext']\n",
      "start to process table spatialite_history, indicators: ['spatialite_history.event_id', 'spatialite_history.table_name', 'spatialite_history.geometry_column', 'spatialite_history.event', 'spatialite_history.timestamp', 'spatialite_history.ver_sqlite', 'spatialite_history.ver_splite']\n",
      "start to process table sqlite_sequence, indicators: ['sqlite_sequence.name', 'sqlite_sequence.seq']\n",
      "start to process table geometry_columns, indicators: ['geometry_columns.f_table_name', 'geometry_columns.f_geometry_column', 'geometry_columns.geometry_type', 'geometry_columns.coord_dimension', 'geometry_columns.srid', 'geometry_columns.spatial_index_enabled']\n",
      "start to process table spatial_ref_sys_aux, indicators: ['spatial_ref_sys_aux.srid', 'spatial_ref_sys_aux.is_geographic', 'spatial_ref_sys_aux.has_flipped_axes', 'spatial_ref_sys_aux.spheroid', 'spatial_ref_sys_aux.prime_meridian', 'spatial_ref_sys_aux.datum', 'spatial_ref_sys_aux.projection', 'spatial_ref_sys_aux.unit', 'spatial_ref_sys_aux.axis_1_name', 'spatial_ref_sys_aux.axis_1_orientation', 'spatial_ref_sys_aux.axis_2_name', 'spatial_ref_sys_aux.axis_2_orientation']\n",
      "start to process table views_geometry_columns, indicators: ['views_geometry_columns.view_name', 'views_geometry_columns.view_geometry', 'views_geometry_columns.view_rowid', 'views_geometry_columns.f_table_name', 'views_geometry_columns.f_geometry_column', 'views_geometry_columns.read_only']\n",
      "start to process table virts_geometry_columns, indicators: ['virts_geometry_columns.virt_name', 'virts_geometry_columns.virt_geometry', 'virts_geometry_columns.geometry_type', 'virts_geometry_columns.coord_dimension', 'virts_geometry_columns.srid']\n",
      "start to process table geometry_columns_statistics, indicators: ['geometry_columns_statistics.f_table_name', 'geometry_columns_statistics.f_geometry_column', 'geometry_columns_statistics.last_verified', 'geometry_columns_statistics.row_count', 'geometry_columns_statistics.extent_min_x', 'geometry_columns_statistics.extent_min_y', 'geometry_columns_statistics.extent_max_x', 'geometry_columns_statistics.extent_max_y']\n",
      "start to process table views_geometry_columns_statistics, indicators: ['views_geometry_columns_statistics.view_name', 'views_geometry_columns_statistics.view_geometry', 'views_geometry_columns_statistics.last_verified', 'views_geometry_columns_statistics.row_count', 'views_geometry_columns_statistics.extent_min_x', 'views_geometry_columns_statistics.extent_min_y', 'views_geometry_columns_statistics.extent_max_x', 'views_geometry_columns_statistics.extent_max_y']\n",
      "start to process table virts_geometry_columns_statistics, indicators: ['virts_geometry_columns_statistics.virt_name', 'virts_geometry_columns_statistics.virt_geometry', 'virts_geometry_columns_statistics.last_verified', 'virts_geometry_columns_statistics.row_count', 'virts_geometry_columns_statistics.extent_min_x', 'virts_geometry_columns_statistics.extent_min_y', 'virts_geometry_columns_statistics.extent_max_x', 'virts_geometry_columns_statistics.extent_max_y']\n",
      "start to process table geometry_columns_field_infos, indicators: ['geometry_columns_field_infos.f_table_name', 'geometry_columns_field_infos.f_geometry_column', 'geometry_columns_field_infos.ordinal', 'geometry_columns_field_infos.column_name', 'geometry_columns_field_infos.null_values', 'geometry_columns_field_infos.integer_values', 'geometry_columns_field_infos.double_values', 'geometry_columns_field_infos.text_values', 'geometry_columns_field_infos.blob_values', 'geometry_columns_field_infos.max_size', 'geometry_columns_field_infos.integer_min', 'geometry_columns_field_infos.integer_max', 'geometry_columns_field_infos.double_min', 'geometry_columns_field_infos.double_max']\n",
      "start to process table views_geometry_columns_field_infos, indicators: ['views_geometry_columns_field_infos.view_name', 'views_geometry_columns_field_infos.view_geometry', 'views_geometry_columns_field_infos.ordinal', 'views_geometry_columns_field_infos.column_name', 'views_geometry_columns_field_infos.null_values', 'views_geometry_columns_field_infos.integer_values', 'views_geometry_columns_field_infos.double_values', 'views_geometry_columns_field_infos.text_values', 'views_geometry_columns_field_infos.blob_values', 'views_geometry_columns_field_infos.max_size', 'views_geometry_columns_field_infos.integer_min', 'views_geometry_columns_field_infos.integer_max', 'views_geometry_columns_field_infos.double_min', 'views_geometry_columns_field_infos.double_max']\n",
      "start to process table virts_geometry_columns_field_infos, indicators: ['virts_geometry_columns_field_infos.virt_name', 'virts_geometry_columns_field_infos.virt_geometry', 'virts_geometry_columns_field_infos.ordinal', 'virts_geometry_columns_field_infos.column_name', 'virts_geometry_columns_field_infos.null_values', 'virts_geometry_columns_field_infos.integer_values', 'virts_geometry_columns_field_infos.double_values', 'virts_geometry_columns_field_infos.text_values', 'virts_geometry_columns_field_infos.blob_values', 'virts_geometry_columns_field_infos.max_size', 'virts_geometry_columns_field_infos.integer_min', 'virts_geometry_columns_field_infos.integer_max', 'virts_geometry_columns_field_infos.double_min', 'virts_geometry_columns_field_infos.double_max']\n",
      "start to process table geometry_columns_time, indicators: ['geometry_columns_time.f_table_name', 'geometry_columns_time.f_geometry_column', 'geometry_columns_time.last_insert', 'geometry_columns_time.last_update', 'geometry_columns_time.last_delete']\n",
      "start to process table geometry_columns_auth, indicators: ['geometry_columns_auth.f_table_name', 'geometry_columns_auth.f_geometry_column', 'geometry_columns_auth.read_only', 'geometry_columns_auth.hidden']\n",
      "start to process table views_geometry_columns_auth, indicators: ['views_geometry_columns_auth.view_name', 'views_geometry_columns_auth.view_geometry', 'views_geometry_columns_auth.hidden']\n",
      "start to process table virts_geometry_columns_auth, indicators: ['virts_geometry_columns_auth.virt_name', 'virts_geometry_columns_auth.virt_geometry', 'virts_geometry_columns_auth.hidden']\n",
      "start to process table sql_statements_log, indicators: ['sql_statements_log.id', 'sql_statements_log.time_start', 'sql_statements_log.time_end', 'sql_statements_log.user_agent', 'sql_statements_log.sql_statement', 'sql_statements_log.success', 'sql_statements_log.error_cause']\n",
      "start to process table Fires, indicators: ['Fires.OBJECTID', 'Fires.FOD_ID', 'Fires.FPA_ID', 'Fires.SOURCE_SYSTEM_TYPE', 'Fires.SOURCE_SYSTEM', 'Fires.NWCG_REPORTING_AGENCY', 'Fires.NWCG_REPORTING_UNIT_ID', 'Fires.NWCG_REPORTING_UNIT_NAME', 'Fires.SOURCE_REPORTING_UNIT', 'Fires.SOURCE_REPORTING_UNIT_NAME', 'Fires.LOCAL_FIRE_REPORT_ID', 'Fires.LOCAL_INCIDENT_ID', 'Fires.FIRE_CODE', 'Fires.FIRE_NAME', 'Fires.ICS_209_INCIDENT_NUMBER', 'Fires.ICS_209_NAME', 'Fires.MTBS_ID', 'Fires.MTBS_FIRE_NAME', 'Fires.COMPLEX_NAME', 'Fires.FIRE_YEAR', 'Fires.DISCOVERY_DATE', 'Fires.DISCOVERY_DOY', 'Fires.DISCOVERY_TIME', 'Fires.STAT_CAUSE_CODE', 'Fires.STAT_CAUSE_DESCR', 'Fires.CONT_DATE', 'Fires.CONT_DOY', 'Fires.CONT_TIME', 'Fires.FIRE_SIZE', 'Fires.FIRE_SIZE_CLASS', 'Fires.LATITUDE', 'Fires.LONGITUDE', 'Fires.OWNER_CODE', 'Fires.OWNER_DESCR', 'Fires.STATE', 'Fires.COUNTY', 'Fires.FIPS_CODE', 'Fires.FIPS_NAME', 'Fires.Shape']\n",
      "start to process table idx_Fires_Shape, indicators: ['idx_Fires_Shape.pkid', 'idx_Fires_Shape.xmin', 'idx_Fires_Shape.xmax', 'idx_Fires_Shape.ymin', 'idx_Fires_Shape.ymax']\n",
      "start to process table idx_Fires_Shape_node, indicators: ['idx_Fires_Shape_node.nodeno', 'idx_Fires_Shape_node.data']\n",
      "start to process table idx_Fires_Shape_rowid, indicators: ['idx_Fires_Shape_rowid.rowid', 'idx_Fires_Shape_rowid.nodeno']\n",
      "start to process table idx_Fires_Shape_parent, indicators: ['idx_Fires_Shape_parent.nodeno', 'idx_Fires_Shape_parent.parentnode']\n",
      "start to process table NWCG_UnitIDActive_20170109, indicators: ['NWCG_UnitIDActive_20170109.OBJECTID', 'NWCG_UnitIDActive_20170109.UnitId', 'NWCG_UnitIDActive_20170109.GeographicArea', 'NWCG_UnitIDActive_20170109.Gacc', 'NWCG_UnitIDActive_20170109.WildlandRole', 'NWCG_UnitIDActive_20170109.UnitType', 'NWCG_UnitIDActive_20170109.Department', 'NWCG_UnitIDActive_20170109.Agency', 'NWCG_UnitIDActive_20170109.Parent', 'NWCG_UnitIDActive_20170109.Country', 'NWCG_UnitIDActive_20170109.State', 'NWCG_UnitIDActive_20170109.Code', 'NWCG_UnitIDActive_20170109.Name']\n",
      "start to process wwe\n",
      "database wwe description file already exists, jump to next database\n",
      "start to process yeast\n",
      "database yeast description file already exists, jump to next database\n",
      "start to process yelp\n",
      "database yelp description file already exists, jump to next database\n",
      "start to process Accidents\n",
      "database Accidents description file already exists, jump to next database\n",
      "start to process AdventureWorks2014\n",
      "database AdventureWorks2014 description file already exists, jump to next database\n",
      "start to process Atherosclerosis\n",
      "database Atherosclerosis description file already exists, jump to next database\n",
      "start to process Basketball_men\n",
      "database Basketball_men description file already exists, jump to next database\n",
      "start to process Basketball_women\n",
      "database Basketball_women description file already exists, jump to next database\n",
      "start to process classicmodels\n",
      "database classicmodels description file already exists, jump to next database\n",
      "start to process ErgastF1\n",
      "database ErgastF1 description file already exists, jump to next database\n",
      "start to process financial\n",
      "Error: near \"order\": syntax error\n",
      "start to process table account, indicators: ['account.account_id', 'account.district_id', 'account.frequency', 'account.date']\n",
      "start to process table card, indicators: ['card.card_id', 'card.disp_id', 'card.type', 'card.issued']\n",
      "start to process table client, indicators: ['client.client_id', 'client.gender', 'client.birth_date', 'client.district_id']\n",
      "start to process table disp, indicators: ['disp.disp_id', 'disp.client_id', 'disp.account_id', 'disp.type']\n",
      "start to process table district, indicators: ['district.district_id', 'district.A2', 'district.A3', 'district.A4', 'district.A5', 'district.A6', 'district.A7', 'district.A8', 'district.A9', 'district.A10', 'district.A11', 'district.A12', 'district.A13', 'district.A14', 'district.A15', 'district.A16']\n",
      "start to process table loan, indicators: ['loan.loan_id', 'loan.account_id', 'loan.date', 'loan.amount', 'loan.duration', 'loan.payments', 'loan.status']\n",
      "start to process table trans, indicators: ['trans.trans_id', 'trans.account_id', 'trans.date', 'trans.type', 'trans.operation', 'trans.amount', 'trans.balance', 'trans.k_symbol', 'trans.bank', 'trans.account']\n",
      "start to process geneea\n",
      "database geneea description file already exists, jump to next database\n",
      "start to process Grants\n",
      "database Grants description file already exists, jump to next database\n",
      "start to process Hockey\n",
      "database Hockey description file already exists, jump to next database\n",
      "start to process lahman_2014\n",
      "database lahman_2014 description file already exists, jump to next database\n",
      "start to process legalActs\n",
      "database legalActs description file already exists, jump to next database\n",
      "start to process medical\n",
      "database medical description file already exists, jump to next database\n",
      "start to process Mondial_geo\n",
      "database Mondial_geo description file already exists, jump to next database\n",
      "start to process NCAA\n",
      "database NCAA description file already exists, jump to next database\n",
      "start to process northwind\n",
      "Error: near \"order\": syntax error\n",
      "start to process table categories, indicators: ['categories.CategoryID', 'categories.CategoryName', 'categories.Description', 'categories.Picture']\n",
      "start to process table sqlite_sequence, indicators: ['sqlite_sequence.name', 'sqlite_sequence.seq']\n",
      "start to process table customercustomerdemo, indicators: ['customercustomerdemo.CustomerID', 'customercustomerdemo.CustomerTypeID']\n",
      "start to process table customerdemographics, indicators: ['customerdemographics.CustomerTypeID', 'customerdemographics.CustomerDesc']\n",
      "start to process table customers, indicators: ['customers.CustomerID', 'customers.CompanyName', 'customers.ContactName', 'customers.ContactTitle', 'customers.Address', 'customers.City', 'customers.Region', 'customers.PostalCode', 'customers.Country', 'customers.Phone', 'customers.Fax']\n",
      "start to process table employees, indicators: ['employees.EmployeeID', 'employees.LastName', 'employees.FirstName', 'employees.Title', 'employees.TitleOfCourtesy', 'employees.BirthDate', 'employees.HireDate', 'employees.Address', 'employees.City', 'employees.Region', 'employees.PostalCode', 'employees.Country', 'employees.HomePhone', 'employees.Extension', 'employees.Photo', 'employees.Notes', 'employees.ReportsTo', 'employees.PhotoPath', 'employees.Salary']\n",
      "start to process table employeeterritories, indicators: ['employeeterritories.EmployeeID', 'employeeterritories.TerritoryID']\n",
      "start to process table orders, indicators: ['orders.OrderID', 'orders.CustomerID', 'orders.EmployeeID', 'orders.OrderDate', 'orders.RequiredDate', 'orders.ShippedDate', 'orders.ShipVia', 'orders.Freight', 'orders.ShipName', 'orders.ShipAddress', 'orders.ShipCity', 'orders.ShipRegion', 'orders.ShipPostalCode', 'orders.ShipCountry']\n",
      "start to process table products, indicators: ['products.ProductID', 'products.ProductName', 'products.SupplierID', 'products.CategoryID', 'products.QuantityPerUnit', 'products.UnitPrice', 'products.UnitsInStock', 'products.UnitsOnOrder', 'products.ReorderLevel', 'products.Discontinued']\n",
      "start to process table region, indicators: ['region.RegionID', 'region.RegionDescription']\n",
      "start to process table shippers, indicators: ['shippers.ShipperID', 'shippers.CompanyName', 'shippers.Phone']\n",
      "start to process table suppliers, indicators: ['suppliers.SupplierID', 'suppliers.CompanyName', 'suppliers.ContactName', 'suppliers.ContactTitle', 'suppliers.Address', 'suppliers.City', 'suppliers.Region', 'suppliers.PostalCode', 'suppliers.Country', 'suppliers.Phone', 'suppliers.Fax', 'suppliers.HomePage']\n",
      "start to process table territories, indicators: ['territories.TerritoryID', 'territories.TerritoryDescription', 'territories.RegionID']\n",
      "start to process stats\n",
      "start to process table badges, indicators: ['badges.Id', 'badges.UserId', 'badges.Name', 'badges.Date']\n",
      "start to process table comments, indicators: ['comments.Id', 'comments.PostId', 'comments.Score', 'comments.Text', 'comments.CreationDate', 'comments.UserId', 'comments.UserDisplayName']\n",
      "start to process table posthistory, indicators: ['posthistory.Id', 'posthistory.PostHistoryTypeId', 'posthistory.PostId', 'posthistory.RevisionGUID', 'posthistory.CreationDate', 'posthistory.UserId', 'posthistory.Text', 'posthistory.Comment', 'posthistory.UserDisplayName']\n",
      "start to process table postlinks, indicators: ['postlinks.Id', 'postlinks.CreationDate', 'postlinks.PostId', 'postlinks.RelatedPostId', 'postlinks.LinkTypeId']\n",
      "start to process table posts, indicators: ['posts.Id', 'posts.PostTypeId', 'posts.AcceptedAnswerId', 'posts.CreaionDate', 'posts.Score', 'posts.ViewCount', 'posts.Body', 'posts.OwnerUserId', 'posts.LasActivityDate', 'posts.Title', 'posts.Tags', 'posts.AnswerCount', 'posts.CommentCount', 'posts.FavoriteCount', 'posts.LastEditorUserId', 'posts.LastEditDate', 'posts.CommunityOwnedDate', 'posts.ParentId', 'posts.ClosedDate', 'posts.OwnerDisplayName', 'posts.LastEditorDisplayName']\n",
      "start to process table tags, indicators: ['tags.Id', 'tags.TagName', 'tags.Count', 'tags.ExcerptPostId', 'tags.WikiPostId']\n",
      "start to process table users, indicators: ['users.Id', 'users.Reputation', 'users.CreationDate', 'users.DisplayName', 'users.LastAccessDate', 'users.WebsiteUrl', 'users.Location', 'users.AboutMe', 'users.Views', 'users.UpVotes', 'users.DownVotes', 'users.AccountId', 'users.Age', 'users.ProfileImageUrl']\n",
      "start to process table votes, indicators: ['votes.Id', 'votes.PostId', 'votes.VoteTypeId', 'votes.CreationDate', 'votes.UserId', 'votes.BountyAmount']\n",
      "start to process tpcc\n",
      "start to process table c_customer, indicators: ['c_customer.c_id', 'c_customer.c_d_id', 'c_customer.c_w_id', 'c_customer.c_first', 'c_customer.c_middle', 'c_customer.c_last', 'c_customer.c_street_1', 'c_customer.c_street_2', 'c_customer.c_city', 'c_customer.c_state', 'c_customer.c_zip', 'c_customer.c_phone', 'c_customer.c_since', 'c_customer.c_credit', 'c_customer.c_credit_lim', 'c_customer.c_discount', 'c_customer.c_balance', 'c_customer.c_ytd_payment', 'c_customer.c_payment_cnt', 'c_customer.c_delivery_cnt', 'c_customer.c_data1', 'c_customer.c_data2']\n",
      "start to process table c_district, indicators: ['c_district.d_id', 'c_district.d_w_id', 'c_district.d_name', 'c_district.d_street_1', 'c_district.d_street_2', 'c_district.d_city', 'c_district.d_state', 'c_district.d_zip', 'c_district.d_tax', 'c_district.d_ytd', 'c_district.d_next_o_id']\n",
      "start to process table c_history, indicators: ['c_history.h_c_id', 'c_history.h_c_d_id', 'c_history.h_c_w_id', 'c_history.h_d_id', 'c_history.h_w_id', 'c_history.h_date', 'c_history.h_amount', 'c_history.h_data']\n",
      "start to process table c_item, indicators: ['c_item.i_id', 'c_item.i_im_id', 'c_item.i_name', 'c_item.i_price', 'c_item.i_data']\n",
      "start to process table c_new_order, indicators: ['c_new_order.no_o_id', 'c_new_order.no_d_id', 'c_new_order.no_w_id']\n",
      "start to process table c_order, indicators: ['c_order.o_id', 'c_order.o_d_id', 'c_order.o_w_id', 'c_order.o_c_id', 'c_order.o_entry_d', 'c_order.o_carrier_id', 'c_order.o_ol_cnt', 'c_order.o_all_local']\n",
      "start to process table c_order_line, indicators: ['c_order_line.ol_o_id', 'c_order_line.ol_d_id', 'c_order_line.ol_w_id', 'c_order_line.ol_number', 'c_order_line.ol_i_id', 'c_order_line.ol_supply_w_id', 'c_order_line.ol_delivery_d', 'c_order_line.ol_quantity', 'c_order_line.ol_amount', 'c_order_line.ol_dist_info']\n",
      "start to process table c_stock, indicators: ['c_stock.s_i_id', 'c_stock.s_w_id', 'c_stock.s_quantity', 'c_stock.s_dist_01', 'c_stock.s_dist_02', 'c_stock.s_dist_03', 'c_stock.s_dist_04', 'c_stock.s_dist_05', 'c_stock.s_dist_06', 'c_stock.s_dist_07', 'c_stock.s_dist_08', 'c_stock.s_dist_09', 'c_stock.s_dist_10', 'c_stock.s_ytd', 'c_stock.s_order_cnt', 'c_stock.s_remote_cnt', 'c_stock.s_data']\n",
      "start to process table c_warehouse, indicators: ['c_warehouse.w_id', 'c_warehouse.w_name', 'c_warehouse.w_street_1', 'c_warehouse.w_street_2', 'c_warehouse.w_city', 'c_warehouse.w_state', 'c_warehouse.w_zip', 'c_warehouse.w_tax', 'c_warehouse.w_ytd']\n",
      "start to process voc\n",
      "start to process table craftsmen, indicators: ['craftsmen.number', 'craftsmen.number_sup', 'craftsmen.trip', 'craftsmen.trip_sup', 'craftsmen.onboard_at_departure', 'craftsmen.death_at_cape', 'craftsmen.left_at_cape', 'craftsmen.onboard_at_cape', 'craftsmen.death_during_voyage', 'craftsmen.onboard_at_arrival']\n",
      "start to process table impotenten, indicators: ['impotenten.number', 'impotenten.number_sup', 'impotenten.trip', 'impotenten.trip_sup', 'impotenten.onboard_at_departure', 'impotenten.death_at_cape', 'impotenten.left_at_cape', 'impotenten.onboard_at_cape', 'impotenten.death_during_voyage', 'impotenten.onboard_at_arrival']\n",
      "start to process table invoices, indicators: ['invoices.number', 'invoices.number_sup', 'invoices.trip', 'invoices.trip_sup', 'invoices.invoice', 'invoices.chamber']\n",
      "start to process table passengers, indicators: ['passengers.number', 'passengers.number_sup', 'passengers.trip', 'passengers.trip_sup', 'passengers.onboard_at_departure', 'passengers.death_at_cape', 'passengers.left_at_cape', 'passengers.onboard_at_cape', 'passengers.death_during_voyage', 'passengers.onboard_at_arrival']\n",
      "start to process table seafarers, indicators: ['seafarers.number', 'seafarers.number_sup', 'seafarers.trip', 'seafarers.trip_sup', 'seafarers.onboard_at_departure', 'seafarers.death_at_cape', 'seafarers.left_at_cape', 'seafarers.onboard_at_cape', 'seafarers.death_during_voyage', 'seafarers.onboard_at_arrival']\n",
      "start to process table soldiers, indicators: ['soldiers.number', 'soldiers.number_sup', 'soldiers.trip', 'soldiers.trip_sup', 'soldiers.onboard_at_departure', 'soldiers.death_at_cape', 'soldiers.left_at_cape', 'soldiers.onboard_at_cape', 'soldiers.death_during_voyage', 'soldiers.onboard_at_arrival']\n",
      "start to process table total, indicators: ['total.number', 'total.number_sup', 'total.trip', 'total.trip_sup', 'total.onboard_at_departure', 'total.death_at_cape', 'total.left_at_cape', 'total.onboard_at_cape', 'total.death_during_voyage', 'total.onboard_at_arrival']\n",
      "start to process table voyages, indicators: ['voyages.artificial_id', 'voyages.number', 'voyages.number_sup', 'voyages.trip', 'voyages.trip_sup', 'voyages.boatname', 'voyages.master', 'voyages.tonnage', 'voyages.type_of_boat', 'voyages.built', 'voyages.bought', 'voyages.hired', 'voyages.yard', 'voyages.chamber', 'voyages.departure_date', 'voyages.departure_harbour', 'voyages.cape_arrival', 'voyages.cape_departure', 'voyages.cape_call', 'voyages.arrival_date', 'voyages.arrival_harbour', 'voyages.next_voyage', 'voyages.particulars']\n"
     ]
    }
   ],
   "source": [
    "from src.qa import makecomversation\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "from openai import APIConnectionError, OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=config['api']['key']\n",
    ")\n",
    "\n",
    "def get_table_descriptions(structure_info, db_description, table_indicators): ## from gpt\n",
    "    prompt = f\"\"\"You are a database design expert. \n",
    "    You need to guess the meaning of the provided fields based on the provided database structure information and a description of the database.\n",
    "    the database description is:\n",
    "    {db_description}\n",
    "\n",
    "    The database structure information is:\n",
    "    {structure_info}\n",
    "    \"\"\"\n",
    "    usercontent = f\"\"\"You need to guess the meaning of each field in the table provided below, and then add a description for each field separately. \n",
    "    Make sure there are no omissions.\n",
    "    \n",
    "    The table contains the following fields:\n",
    "    {table_indicators}\n",
    "    \"\"\"\n",
    "    \n",
    "    return_format = '[{\"table indicator\": \"the meaning of the indicator\"}, {\"table indicator\": \"the meaning of the indicator\"}, ...]'\n",
    "    \n",
    "    result = makecomversation(prompt, usercontent, return_format, client)\n",
    "    return result\n",
    "\n",
    "\n",
    "with open('datasource.json', 'r') as file:\n",
    "        urls = json.load(file)\n",
    "\n",
    "results = {}\n",
    "for item in urls:\n",
    "    if 'description' not in item:\n",
    "        print(f\"database {item['db_name']} has no description, jump to next database\")\n",
    "        print(item)\n",
    "        continue\n",
    "    \n",
    "    db_name = item['db_name']\n",
    "    print(f\"start to process {db_name}\")\n",
    "    db_file = os.path.join('./database/', f'{db_name}.sqlite')\n",
    "    if not os.path.exists(db_file):\n",
    "        print(f\"database file {db_file} not exists, jump to next database\")\n",
    "        continue\n",
    "    \n",
    "    # 如果目录 f'./database_descriptions/' 该f'{db_name}_descriptions.csv' 已经存在了就跳过\n",
    "    if os.path.exists(f'./database_descriptions/{db_name}_descriptions.csv'):\n",
    "        print(f\"database {db_name} description file already exists, jump to next database\")\n",
    "        continue\n",
    "\n",
    "    tables_info, structure_description = extract_tables_and_columns(db_file)\n",
    "    if not tables_info:\n",
    "        print(f\"database {db_name} extract tables and columns failed, jump to next database\")\n",
    "        continue\n",
    "\n",
    "    table_descriptions = {}\n",
    "    for table_name in tables_info:\n",
    "        columns = tables_info[table_name]\n",
    "        table_indicators = []\n",
    "        for column in columns:\n",
    "            indicator = f\"{table_name}.{column[0]}\"\n",
    "            table_indicators.append(indicator)\n",
    "        print(f\"start to process table {table_name}, indicators: {table_indicators}\")\n",
    "        table_des = get_table_descriptions(structure_description, item['description'], table_indicators)\n",
    "        table_descriptions[table_name] = table_des\n",
    "    \n",
    "    table_list = []\n",
    "    column_list = []\n",
    "    description_list = []\n",
    "\n",
    "    for table, columns in table_descriptions.items():\n",
    "        for column, description in columns.items():\n",
    "            # bug:??\n",
    "            column = column.split('.')[-1]\n",
    "            table_list.append(table)\n",
    "            column_list.append(column)\n",
    "            description_list.append(description)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'table': table_list,\n",
    "        'column': column_list,\n",
    "        'description': description_list\n",
    "    })\n",
    "\n",
    "    df.to_csv(f'./database_descriptions/{db_name}_descriptions.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.qa import makecomversation\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    return config\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "from openai import APIConnectionError, OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=config['api']['key']\n",
    ")\n",
    "\n",
    "def get_descriptions(structure_info, db_description, indicators): ## from gpt\n",
    "    prompt = f\"\"\"You are a database design expert. \n",
    "    You need to guess the meaning of the provided fields based on the provided database structure information and a description of the database.\n",
    "    the database description is:\n",
    "    {db_description}\n",
    "\n",
    "    The database structure information is:\n",
    "    {structure_info}\n",
    "    \"\"\"\n",
    "    usercontent = f\"\"\"You need to guess the meaning of each field in the table provided below, and then add a description for each field separately. \n",
    "    Make sure there are no omissions.\n",
    "    \n",
    "    The table contains the following fields:\n",
    "    {indicators}\n",
    "    \"\"\"\n",
    "    \n",
    "    return_format = '[{\"table indicator\": \"the meaning of the indicator\"}, {\"table indicator\": \"the meaning of the indicator\"}, ...]'\n",
    "    \n",
    "    result = makecomversation(prompt, usercontent, return_format, client)\n",
    "    return result\n",
    "\n",
    "\n",
    "with open('datasource.json', 'r') as file:\n",
    "        urls = json.load(file)\n",
    "\n",
    "results = {}\n",
    "for item in urls:\n",
    "    if 'description' not in item:\n",
    "        print(f\"database {item['db_name']} has no description, jump to next database\")\n",
    "        print(item)\n",
    "        continue\n",
    "    \n",
    "    db_name = item['db_name']\n",
    "    print(f\"start to process {db_name}\")\n",
    "    db_file = os.path.join('./database/', f'{db_name}.sqlite')\n",
    "    if not os.path.exists(db_file):\n",
    "        print(f\"database file {db_file} not exists, jump to next database\")\n",
    "        continue\n",
    "    \n",
    "    # 如果目录 f'./database_descriptions/' 该f'{db_name}_descriptions.csv' 已经存在了就跳过\n",
    "    if os.path.exists(f'./database_descriptions/{db_name}_descriptions.csv'):\n",
    "        print(f\"database {db_name} description file already exists, jump to next database\")\n",
    "        continue\n",
    "\n",
    "    tables_info, structure_description = extract_tables_and_columns(db_file)\n",
    "    if not tables_info:\n",
    "        print(f\"database {db_name} extract tables and columns failed, jump to next database\")\n",
    "        continue\n",
    "\n",
    "    table_descriptions = {}\n",
    "    for table_name in tables_info:\n",
    "        columns = tables_info[table_name]\n",
    "        table_indicators = []\n",
    "        for column in columns:\n",
    "            indicator = f\"{table_name}.{column[0]}\"\n",
    "            table_indicators.append(indicator)\n",
    "        table_des = get_table_descriptions(structure_description, item['description'], table_indicators)\n",
    "        table_descriptions[table_name] = table_des\n",
    "    table_list = []\n",
    "    column_list = []\n",
    "    description_list = []\n",
    "\n",
    "    for table, columns in table_descriptions.items():\n",
    "        for column, description in columns.items():\n",
    "            # bug:??\n",
    "            column = column.split('.')[-1]\n",
    "            table_list.append(table)\n",
    "            column_list.append(column)\n",
    "            description_list.append(description)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'table': table_list,\n",
    "        'column': column_list,\n",
    "        'description': description_list\n",
    "    })\n",
    "\n",
    "    df.to_csv(f'./database_descriptions/{db_name}_descriptions.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['column natl_tv_broadcaster_abbreviation in table game_summary is The abbreviation of the national TV broadcaster',\n",
       " 'column game_date_est in table game_summary is The date of the game in Eastern Standard Time']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debuging\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sql = 'SELECT game_summary.natl_tv_broadcaster_abbreviation FROM game_summary AS game_summary ORDER BY game_summary.game_date_est DESC'\n",
    "\n",
    "# 提取sql相关字段\n",
    "def get_indicators(clause_list):\n",
    "  pattern = r'\\b\\w+\\.\\w+\\b'\n",
    "  sql_indicators = []\n",
    "  for clause in clause_list:\n",
    "    matches = re.findall(pattern, clause)\n",
    "    sql_indicators += matches\n",
    "  return sql_indicators # [table.column, table.column, ...]\n",
    "\n",
    "\n",
    "# 获取字段描述\n",
    "def get_sql_descriptions(cluse_list, db_name = None):\n",
    "    if db_name is None:\n",
    "        return 'no descriptions available, you have to guess from the table name.'\n",
    "    \n",
    "    des_path = f'./database_descriptions/{db_name}_descriptions.csv'\n",
    "\n",
    "    if not os.path.exists(des_path):\n",
    "        return 'no descriptions available, you have to guess from the table name.'\n",
    "    \n",
    "    descriptions = pd.read_csv(des_path)\n",
    "\n",
    "\n",
    "    indicators = get_indicators(cluse_list)\n",
    "\n",
    "    relevent_descriptions = []\n",
    "    for indicator in indicators:\n",
    "        table, column = indicator.split('.')\n",
    "        try:\n",
    "            descirption = descriptions[(descriptions['table'] == table) & (descriptions['column'] == column)]['description'].values[0]\n",
    "            relevent_descriptions.append(f'column {column} in table {table} is {descirption}. ')\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return relevent_descriptions\n",
    "\n",
    "\n",
    "sql_descriptions = get_sql_descriptions([sql], 'NBA')\n",
    "\n",
    "sql_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rename ./data/4_22/complex/indicators_2022_complex.json to ./data/4_22/complex/indicators_2022.json\n",
      "rename ./data/4_22/complex/matches_football_complex.json to ./data/4_22/complex/matches_football.json\n",
      "rename ./data/4_22/complex/NBA_complex.json to ./data/4_22/complex/NBA.json\n",
      "rename ./data/4_22/complex/pitchfork_complex.json to ./data/4_22/complex/pitchfork.json\n",
      "rename ./data/4_22/complex/spatiaLite_small_complex.json to ./data/4_22/complex/spatiaLite_small.json\n",
      "rename ./data/4_22/complex/spotify_complex.json to ./data/4_22/complex/spotify.json\n",
      "rename ./data/4_22/complex/tvmaze_complex.json to ./data/4_22/complex/tvmaze.json\n",
      "rename ./data/4_22/complex/UK_arms_complex.json to ./data/4_22/complex/UK_arms.json\n",
      "rename ./data/4_22/complex/WDI_complex.json to ./data/4_22/complex/WDI.json\n",
      "rename ./data/4_22/complex/yeast_complex.json to ./data/4_22/complex/yeast.json\n",
      "rename ./data/4_22/complex/yelp_complex.json to ./data/4_22/complex/yelp.json\n"
     ]
    }
   ],
   "source": [
    "# 遍历4_22 目录下所有文件，将文件名按\"_\"分割后去掉最后一部分，重命名文件名\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk('./data/4_22/complex/'):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if file_path.endswith('.json'):\n",
    "            file_name = file.split('.')[0]\n",
    "            new_name = '_'.join(file_name.split('_')[:-1])\n",
    "            new_path = os.path.join(root, f'{new_name}.json')\n",
    "            print(f\"rename {file_path} to {new_path}\")\n",
    "            os.rename(file_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面是一些测试脚本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['application_test.SK_ID_CURR',\n",
       " 'application_test.NAME_CONTRACT_TYPE',\n",
       " 'application_test.CODE_GENDER',\n",
       " 'application_test.FLAG_OWN_CAR',\n",
       " 'application_test.FLAG_OWN_REALTY',\n",
       " 'application_test.CNT_CHILDREN',\n",
       " 'application_test.AMT_INCOME_TOTAL',\n",
       " 'application_test.AMT_CREDIT',\n",
       " 'application_test.AMT_ANNUITY',\n",
       " 'application_test.AMT_GOODS_PRICE',\n",
       " 'application_test.NAME_TYPE_SUITE',\n",
       " 'application_test.NAME_INCOME_TYPE',\n",
       " 'application_test.NAME_EDUCATION_TYPE',\n",
       " 'application_test.NAME_FAMILY_STATUS',\n",
       " 'application_test.NAME_HOUSING_TYPE',\n",
       " 'application_test.REGION_POPULATION_RELATIVE',\n",
       " 'application_test.DAYS_BIRTH',\n",
       " 'application_test.DAYS_EMPLOYED',\n",
       " 'application_test.DAYS_REGISTRATION',\n",
       " 'application_test.DAYS_ID_PUBLISH',\n",
       " 'application_test.OWN_CAR_AGE',\n",
       " 'application_test.FLAG_MOBIL',\n",
       " 'application_test.FLAG_EMP_PHONE',\n",
       " 'application_test.FLAG_WORK_PHONE',\n",
       " 'application_test.FLAG_CONT_MOBILE',\n",
       " 'application_test.FLAG_PHONE',\n",
       " 'application_test.FLAG_EMAIL',\n",
       " 'application_test.OCCUPATION_TYPE',\n",
       " 'application_test.CNT_FAM_MEMBERS',\n",
       " 'application_test.REGION_RATING_CLIENT',\n",
       " 'application_test.REGION_RATING_CLIENT_W_CITY',\n",
       " 'application_test.WEEKDAY_APPR_PROCESS_START',\n",
       " 'application_test.HOUR_APPR_PROCESS_START',\n",
       " 'application_test.REG_REGION_NOT_LIVE_REGION',\n",
       " 'application_test.REG_REGION_NOT_WORK_REGION',\n",
       " 'application_test.LIVE_REGION_NOT_WORK_REGION',\n",
       " 'application_test.REG_CITY_NOT_LIVE_CITY',\n",
       " 'application_test.REG_CITY_NOT_WORK_CITY',\n",
       " 'application_test.LIVE_CITY_NOT_WORK_CITY',\n",
       " 'application_test.ORGANIZATION_TYPE',\n",
       " 'application_test.EXT_SOURCE_1',\n",
       " 'application_test.EXT_SOURCE_2',\n",
       " 'application_test.EXT_SOURCE_3',\n",
       " 'application_test.APARTMENTS_AVG',\n",
       " 'application_test.BASEMENTAREA_AVG',\n",
       " 'application_test.YEARS_BEGINEXPLUATATION_AVG',\n",
       " 'application_test.YEARS_BUILD_AVG',\n",
       " 'application_test.COMMONAREA_AVG',\n",
       " 'application_test.ELEVATORS_AVG',\n",
       " 'application_test.ENTRANCES_AVG',\n",
       " 'application_test.FLOORSMAX_AVG',\n",
       " 'application_test.FLOORSMIN_AVG',\n",
       " 'application_test.LANDAREA_AVG',\n",
       " 'application_test.LIVINGAPARTMENTS_AVG',\n",
       " 'application_test.LIVINGAREA_AVG',\n",
       " 'application_test.NONLIVINGAPARTMENTS_AVG',\n",
       " 'application_test.NONLIVINGAREA_AVG',\n",
       " 'application_test.APARTMENTS_MODE',\n",
       " 'application_test.BASEMENTAREA_MODE',\n",
       " 'application_test.YEARS_BEGINEXPLUATATION_MODE',\n",
       " 'application_test.YEARS_BUILD_MODE',\n",
       " 'application_test.COMMONAREA_MODE',\n",
       " 'application_test.ELEVATORS_MODE',\n",
       " 'application_test.ENTRANCES_MODE',\n",
       " 'application_test.FLOORSMAX_MODE',\n",
       " 'application_test.FLOORSMIN_MODE',\n",
       " 'application_test.LANDAREA_MODE',\n",
       " 'application_test.LIVINGAPARTMENTS_MODE',\n",
       " 'application_test.LIVINGAREA_MODE',\n",
       " 'application_test.NONLIVINGAPARTMENTS_MODE',\n",
       " 'application_test.NONLIVINGAREA_MODE',\n",
       " 'application_test.APARTMENTS_MEDI',\n",
       " 'application_test.BASEMENTAREA_MEDI',\n",
       " 'application_test.YEARS_BEGINEXPLUATATION_MEDI',\n",
       " 'application_test.YEARS_BUILD_MEDI',\n",
       " 'application_test.COMMONAREA_MEDI',\n",
       " 'application_test.ELEVATORS_MEDI',\n",
       " 'application_test.ENTRANCES_MEDI',\n",
       " 'application_test.FLOORSMAX_MEDI',\n",
       " 'application_test.FLOORSMIN_MEDI',\n",
       " 'application_test.LANDAREA_MEDI',\n",
       " 'application_test.LIVINGAPARTMENTS_MEDI',\n",
       " 'application_test.LIVINGAREA_MEDI',\n",
       " 'application_test.NONLIVINGAPARTMENTS_MEDI',\n",
       " 'application_test.NONLIVINGAREA_MEDI',\n",
       " 'application_test.FONDKAPREMONT_MODE',\n",
       " 'application_test.HOUSETYPE_MODE',\n",
       " 'application_test.TOTALAREA_MODE',\n",
       " 'application_test.WALLSMATERIAL_MODE',\n",
       " 'application_test.EMERGENCYSTATE_MODE',\n",
       " 'application_test.OBS_30_CNT_SOCIAL_CIRCLE',\n",
       " 'application_test.DEF_30_CNT_SOCIAL_CIRCLE',\n",
       " 'application_test.OBS_60_CNT_SOCIAL_CIRCLE',\n",
       " 'application_test.DEF_60_CNT_SOCIAL_CIRCLE',\n",
       " 'application_test.DAYS_LAST_PHONE_CHANGE',\n",
       " 'application_test.FLAG_DOCUMENT_2',\n",
       " 'application_test.FLAG_DOCUMENT_3',\n",
       " 'application_test.FLAG_DOCUMENT_4',\n",
       " 'application_test.FLAG_DOCUMENT_5',\n",
       " 'application_test.FLAG_DOCUMENT_6',\n",
       " 'application_test.FLAG_DOCUMENT_7',\n",
       " 'application_test.FLAG_DOCUMENT_8',\n",
       " 'application_test.FLAG_DOCUMENT_9',\n",
       " 'application_test.FLAG_DOCUMENT_10',\n",
       " 'application_test.FLAG_DOCUMENT_11',\n",
       " 'application_test.FLAG_DOCUMENT_12',\n",
       " 'application_test.FLAG_DOCUMENT_13',\n",
       " 'application_test.FLAG_DOCUMENT_14',\n",
       " 'application_test.FLAG_DOCUMENT_15',\n",
       " 'application_test.FLAG_DOCUMENT_16',\n",
       " 'application_test.FLAG_DOCUMENT_17',\n",
       " 'application_test.FLAG_DOCUMENT_18',\n",
       " 'application_test.FLAG_DOCUMENT_19',\n",
       " 'application_test.FLAG_DOCUMENT_20',\n",
       " 'application_test.FLAG_DOCUMENT_21',\n",
       " 'application_test.AMT_REQ_CREDIT_BUREAU_HOUR',\n",
       " 'application_test.AMT_REQ_CREDIT_BUREAU_DAY',\n",
       " 'application_test.AMT_REQ_CREDIT_BUREAU_WEEK',\n",
       " 'application_test.AMT_REQ_CREDIT_BUREAU_MON',\n",
       " 'application_test.AMT_REQ_CREDIT_BUREAU_QRT',\n",
       " 'application_test.AMT_REQ_CREDIT_BUREAU_YEAR']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_name = 'home_credit_default'\n",
    "db_file = os.path.join('./database/', f'{db_name}.sqlite')\n",
    "tables_info, structure_description = extract_tables_and_columns(db_file)\n",
    "table_name = 'application_test'\n",
    "columns = tables_info[table_name]\n",
    "table_indicators = []\n",
    "for column in columns:\n",
    "    indicator = f\"{table_name}.{column[0]}\"\n",
    "    table_indicators.append(indicator)\n",
    "table_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SK_ID_CURR': 'Unique identifier for the loan application',\n",
       " 'NAME_CONTRACT_TYPE': 'Type of loan cash or revolving',\n",
       " 'CODE_GENDER': 'Gender of the applicant',\n",
       " 'FLAG_OWN_CAR': 'Flag if the applicant owns a car',\n",
       " 'FLAG_OWN_REALTY': 'Flag if the applicant owns real estate',\n",
       " 'CNT_CHILDREN': 'Number of children the applicant has',\n",
       " 'AMT_INCOME_TOTAL': 'Total income of the applicant',\n",
       " 'AMT_CREDIT': 'Credit amount of the loan',\n",
       " 'AMT_ANNUITY': 'Loan annuity',\n",
       " 'AMT_GOODS_PRICE': 'For consumer loans it is the price of the goods for which the loan is given',\n",
       " 'NAME_TYPE_SUITE': 'Who was accompanying client when he was applying for the loan',\n",
       " 'NAME_INCOME_TYPE': 'Clients income type (businessman, working, maternity leave, etc.)',\n",
       " 'NAME_EDUCATION_TYPE': 'Level of highest education the client achieved',\n",
       " 'NAME_FAMILY_STATUS': 'Family status of the client',\n",
       " 'NAME_HOUSING_TYPE': 'What is the housing situation of the client (renting, living with parents, etc.)',\n",
       " 'REGION_POPULATION_RELATIVE': 'Normalized population of region where client lives',\n",
       " 'DAYS_BIRTH': \"Client's age in days at the time of application\",\n",
       " 'DAYS_EMPLOYED': 'How many days before the application the person started current employment',\n",
       " 'DAYS_REGISTRATION': 'How many days before the application did client change his registration',\n",
       " 'DAYS_ID_PUBLISH': 'How many days before the application did client change the identity document with which he applied for the loan',\n",
       " 'OWN_CAR_AGE': \"Age of client's car\",\n",
       " 'FLAG_MOBIL': 'Flag if the client provided a mobile phone',\n",
       " 'FLAG_EMP_PHONE': 'Flag if client provided work phone',\n",
       " 'FLAG_WORK_PHONE': 'Flag if client provided home phone',\n",
       " 'FLAG_CONT_MOBILE': 'Flag if client provided a contact phone',\n",
       " 'FLAG_PHONE': 'Flag if client provided a mobile phone',\n",
       " 'FLAG_EMAIL': 'Flag if client provided an email',\n",
       " 'OCCUPATION_TYPE': 'What kind of occupation does the client have',\n",
       " 'CNT_FAM_MEMBERS': 'How many family members does client have',\n",
       " 'REGION_RATING_CLIENT': 'Our rating of the region where client lives (1,2,3)',\n",
       " 'REGION_RATING_CLIENT_W_CITY': 'Our rating of the region where client lives with taking city into account (1,2,3)',\n",
       " 'WEEKDAY_APPR_PROCESS_START': 'On which day of the week did the client apply for the loan',\n",
       " 'HOUR_APPR_PROCESS_START': 'Approximately at what hour did the client apply for the loan',\n",
       " 'REG_REGION_NOT_LIVE_REGION': \"Flag if client's permanent address does not match contact address\",\n",
       " 'REG_REGION_NOT_WORK_REGION': \"Flag if client's permanent address does not match work address\",\n",
       " 'LIVE_REGION_NOT_WORK_REGION': \"Flag if client's contact address does not match work address\",\n",
       " 'REG_CITY_NOT_LIVE_CITY': \"Flag if client's permanent address does not match contact address\",\n",
       " 'REG_CITY_NOT_WORK_CITY': \"Flag if client's permanent address does not match work address\",\n",
       " 'LIVE_CITY_NOT_WORK_CITY': \"Flag if client's contact address does not match work address\",\n",
       " 'ORGANIZATION_TYPE': 'Type of organization where client works',\n",
       " 'EXT_SOURCE_1': 'Normalized score from external data source',\n",
       " 'EXT_SOURCE_2': 'Normalized score from external data source',\n",
       " 'EXT_SOURCE_3': 'Normalized score from external data source',\n",
       " 'APARTMENTS_AVG': 'Normalized information about building where the client lives',\n",
       " 'BASEMENTAREA_AVG': 'Normalized information about building where the client lives',\n",
       " 'YEARS_BEGINEXPLUATATION_AVG': 'Normalized information about building where the client lives',\n",
       " 'YEARS_BUILD_AVG': 'Normalized information about building where the client lives',\n",
       " 'COMMONAREA_AVG': 'Normalized information about building where the client lives',\n",
       " 'ELEVATORS_AVG': 'Normalized information about building where the client lives',\n",
       " 'ENTRANCES_AVG': 'Normalized information about building where the client lives',\n",
       " 'FLOORSMAX_AVG': 'Normalized information about building where the client lives',\n",
       " 'FLOORSMIN_AVG': 'Normalized information about building where the client lives',\n",
       " 'LANDAREA_AVG': 'Normalized information about building where the client lives',\n",
       " 'LIVINGAPARTMENTS_AVG': 'Normalized information about building where the client lives',\n",
       " 'LIVINGAREA_AVG': 'Normalized information about building where the client lives',\n",
       " 'NONLIVINGAPARTMENTS_AVG': 'Normalized information about building where the client lives',\n",
       " 'NONLIVINGAREA_AVG': 'Normalized information about building where the client lives',\n",
       " 'APARTMENTS_MODE': 'Normalized information about building where the client lives',\n",
       " 'BASEMENTAREA_MODE': 'Normalized information about building where the client lives',\n",
       " 'YEARS_BEGINEXPLUATATION_MODE': 'Normalized information about building where the client lives',\n",
       " 'YEARS_BUILD_MODE': 'Normalized information about building where the client lives',\n",
       " 'COMMONAREA_MODE': 'Normalized information about building where the client lives',\n",
       " 'ELEVATORS_MODE': 'Normalized information about building where the client lives',\n",
       " 'ENTRANCES_MODE': 'Normalized information about building where the client lives',\n",
       " 'FLOORSMAX_MODE': 'Normalized information about building where the client lives',\n",
       " 'FLOORSMIN_MODE': 'Normalized information about building where the client lives',\n",
       " 'LANDAREA_MODE': 'Normalized information about building where the client lives',\n",
       " 'LIVINGAPARTMENTS_MODE': 'Normalized information about building where the client lives',\n",
       " 'LIVINGAREA_MODE': 'Normalized information about building where the client lives',\n",
       " 'NONLIVINGAPARTMENTS_MODE': 'Normalized information about building where the client lives',\n",
       " 'NONLIVINGAREA_MODE': 'Normalized information about building where the client lives',\n",
       " 'APARTMENTS_MEDI': 'Normalized information about building where the client lives',\n",
       " 'BASEMENTAREA_MEDI': 'Normalized information about building where the client lives',\n",
       " 'YEARS_BEGINEXPLUATATION_MEDI': 'Normalized information about building where the client lives',\n",
       " 'YEARS_BUILD_MEDI': 'Normalized information about building where the client lives',\n",
       " 'COMMONAREA_MEDI': 'Normalized information about building where the client lives',\n",
       " 'ELEVATORS_MEDI': 'Normalized information about building where the client lives',\n",
       " 'ENTRANCES_MEDI': 'Normalized information about building where the client lives',\n",
       " 'FLOORSMAX_MEDI': 'Normalized information about building where the client lives',\n",
       " 'FLOORSMIN_MEDI': 'Normalized information about building where the client lives',\n",
       " 'LANDAREA_MEDI': 'Normalized information about building where the client lives',\n",
       " 'LIVINGAPARTMENTS_MEDI': 'Normalized information about building where the client lives',\n",
       " 'LIVINGAREA_MEDI': 'Normalized information about building where the client lives',\n",
       " 'NONLIVINGAPARTMENTS_MEDI': 'Normalized information about building where the client lives',\n",
       " 'NONLIVINGAREA_MEDI': 'Normalized information about building where the client lives',\n",
       " 'FONDKAPREMONT_MODE': 'Normalized information about building where the client lives',\n",
       " 'HOUSETYPE_MODE': 'Normalized information about building where the client lives',\n",
       " 'TOTALAREA_MODE': 'Normalized information about building where the client lives',\n",
       " 'WALLSMATERIAL_MODE': 'Normalized information about building where the client lives',\n",
       " 'EMERGENCYSTATE_MODE': 'Normalized information about building where the client lives',\n",
       " 'OBS_30_CNT_SOCIAL_CIRCLE': \"How many observation of client's social surroundings with observable 30 DPD (days past due) default\",\n",
       " 'DEF_30_CNT_SOCIAL_CIRCLE': \"How many observation of client's social surroundings defaulted on 30 DPD (days past due) default\",\n",
       " 'OBS_60_CNT_SOCIAL_CIRCLE': \"How many observation of client's social surroundings with observable 60 DPD (days past due) default\",\n",
       " 'DEF_60_CNT_SOCIAL_CIRCLE': \"How many observation of client's social surroundings defaulted on 60 (days past due) DPD default\",\n",
       " 'DAYS_LAST_PHONE_CHANGE': 'How many days before application did client change phone',\n",
       " 'FLAG_DOCUMENT_2': 'Did client provide document 2',\n",
       " 'FLAG_DOCUMENT_3': 'Did client provide document 3',\n",
       " 'FLAG_DOCUMENT_4': 'Did client provide document 4',\n",
       " 'FLAG_DOCUMENT_5': 'Did client provide document 5',\n",
       " 'FLAG_DOCUMENT_6': 'Did client provide document 6',\n",
       " 'FLAG_DOCUMENT_7': 'Did client provide document 7',\n",
       " 'FLAG_DOCUMENT_8': 'Did client provide document 8',\n",
       " 'FLAG_DOCUMENT_9': 'Did client provide document 9',\n",
       " 'FLAG_DOCUMENT_10': 'Did client provide document 10',\n",
       " 'FLAG_DOCUMENT_11': 'Did client provide document 11',\n",
       " 'FLAG_DOCUMENT_12': 'Did client provide document 12',\n",
       " 'FLAG_DOCUMENT_13': 'Did client provide document 13',\n",
       " 'FLAG_DOCUMENT_14': 'Did client provide document 14',\n",
       " 'FLAG_DOCUMENT_15': 'Did client provide document 15',\n",
       " 'FLAG_DOCUMENT_16': 'Did client provide document 16',\n",
       " 'FLAG_DOCUMENT_17': 'Did client provide document 17',\n",
       " 'FLAG_DOCUMENT_18': 'Did client provide document 18',\n",
       " 'FLAG_DOCUMENT_19': 'Did client provide document 19',\n",
       " 'FLAG_DOCUMENT_20': 'Did client provide document 20',\n",
       " 'FLAG_DOCUMENT_21': 'Did client provide document 21',\n",
       " 'AMT_REQ_CREDIT_BUREAU_HOUR': 'Number of enquiries to Credit Bureau about the client one hour before application',\n",
       " 'AMT_REQ_CREDIT_BUREAU_DAY': 'Number of enquiries to Credit Bureau about the client one day before application',\n",
       " 'AMT_REQ_CREDIT_BUREAU_WEEK': 'Number of enquiries to Credit Bureau about the client one week before application',\n",
       " 'AMT_REQ_CREDIT_BUREAU_MON': 'Number of enquiries to Credit Bureau about the client one month before application',\n",
       " 'AMT_REQ_CREDIT_BUREAU_QRT': 'Number of enquiries to Credit Bureau about the client one quarter before application',\n",
       " 'AMT_REQ_CREDIT_BUREAU_YEAR': 'Number of enquiries to Credit Bureau about the client one year before application'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "db_description = \"\\n\\napplication_{train|test}.csv\\n\\nThis is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\\nStatic data for all applications. One row represents one loan in our data sample.\\nbureau.csv\\n\\nAll client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\\nFor every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\\nbureau_balance.csv\\n\\nMonthly balances of previous credits in Credit Bureau.\\nThis table has one row for each month of history of every previous credit reported to Credit Bureau \\u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\\nPOS_CASH_balance.csv\\n\\nMonthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \\u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\\ncredit_card_balance.csv\\n\\nMonthly balance snapshots of previous credit cards that the applicant has with Home Credit.\\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \\u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\\nprevious_application.csv\\n\\nAll previous applications for Home Credit loans of clients who have loans in our sample.\\nThere is one row for each previous application related to loans in our data sample.\\ninstallments_payments.csv\\n\\nRepayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\\nThere is a) one row for every payment that was made plus b) one row each for missed payment.\\nOne row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\\nHomeCredit_columns_description.csv\\n\\nThis file contains descriptions for the columns in the various data files.\\n\\n\"\n",
    "table_des = get_table_descriptions(structure_description, db_description, table_indicators)\n",
    "table_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Table</th>\n",
       "      <th>Row</th>\n",
       "      <th>Description</th>\n",
       "      <th>Special</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>SK_ID_CURR</td>\n",
       "      <td>ID of loan in our sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>TARGET</td>\n",
       "      <td>Target variable (1 - client with payment diffi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>NAME_CONTRACT_TYPE</td>\n",
       "      <td>Identification if loan is cash or revolving</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>CODE_GENDER</td>\n",
       "      <td>Gender of the client</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>FLAG_OWN_CAR</td>\n",
       "      <td>Flag if the client owns a car</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>120</td>\n",
       "      <td>application_test</td>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_DAY</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>121</td>\n",
       "      <td>application_test</td>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_WEEK</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>122</td>\n",
       "      <td>application_test</td>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_MON</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>123</td>\n",
       "      <td>application_test</td>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_QRT</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>124</td>\n",
       "      <td>application_test</td>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_YEAR</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                     Table                         Row  \\\n",
       "0             1  application_{train|test}                  SK_ID_CURR   \n",
       "1             2  application_{train|test}                      TARGET   \n",
       "2             5  application_{train|test}          NAME_CONTRACT_TYPE   \n",
       "3             6  application_{train|test}                 CODE_GENDER   \n",
       "4             7  application_{train|test}                FLAG_OWN_CAR   \n",
       "..          ...                       ...                         ...   \n",
       "444         120          application_test   AMT_REQ_CREDIT_BUREAU_DAY   \n",
       "445         121          application_test  AMT_REQ_CREDIT_BUREAU_WEEK   \n",
       "446         122          application_test   AMT_REQ_CREDIT_BUREAU_MON   \n",
       "447         123          application_test   AMT_REQ_CREDIT_BUREAU_QRT   \n",
       "448         124          application_test  AMT_REQ_CREDIT_BUREAU_YEAR   \n",
       "\n",
       "                                           Description Special Unnamed: 5  \\\n",
       "0                             ID of loan in our sample     NaN        NaN   \n",
       "1    Target variable (1 - client with payment diffi...     NaN        NaN   \n",
       "2          Identification if loan is cash or revolving     NaN        NaN   \n",
       "3                                 Gender of the client     NaN        NaN   \n",
       "4                        Flag if the client owns a car     NaN        NaN   \n",
       "..                                                 ...     ...        ...   \n",
       "444  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "445  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "446  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "447  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "448  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "\n",
       "    Unnamed: 6  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "..         ...  \n",
       "444        NaN  \n",
       "445        NaN  \n",
       "446        NaN  \n",
       "447        NaN  \n",
       "448        NaN  \n",
       "\n",
       "[449 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "official_des = pd.read_csv('HomeCredit_columns_description.csv')\n",
    "official_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row</th>\n",
       "      <th>Description_x</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description_y</th>\n",
       "      <th>Special</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SK_ID_CURR</td>\n",
       "      <td>Unique identifier for the loan application</td>\n",
       "      <td>1</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>ID of loan in our sample</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TARGET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>Target variable (1 - client with payment diffi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAME_CONTRACT_TYPE</td>\n",
       "      <td>Type of loan cash or revolving</td>\n",
       "      <td>5</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>Identification if loan is cash or revolving</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CODE_GENDER</td>\n",
       "      <td>Gender of the applicant</td>\n",
       "      <td>6</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>Gender of the client</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FLAG_OWN_CAR</td>\n",
       "      <td>Flag if the applicant owns a car</td>\n",
       "      <td>7</td>\n",
       "      <td>application_{train|test}</td>\n",
       "      <td>Flag if the client owns a car</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_DAY</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>120</td>\n",
       "      <td>application_test</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_WEEK</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>121</td>\n",
       "      <td>application_test</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_MON</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>122</td>\n",
       "      <td>application_test</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_QRT</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>123</td>\n",
       "      <td>application_test</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>AMT_REQ_CREDIT_BUREAU_YEAR</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>124</td>\n",
       "      <td>application_test</td>\n",
       "      <td>Number of enquiries to Credit Bureau about the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Row  \\\n",
       "0                    SK_ID_CURR   \n",
       "1                        TARGET   \n",
       "2            NAME_CONTRACT_TYPE   \n",
       "3                   CODE_GENDER   \n",
       "4                  FLAG_OWN_CAR   \n",
       "..                          ...   \n",
       "444   AMT_REQ_CREDIT_BUREAU_DAY   \n",
       "445  AMT_REQ_CREDIT_BUREAU_WEEK   \n",
       "446   AMT_REQ_CREDIT_BUREAU_MON   \n",
       "447   AMT_REQ_CREDIT_BUREAU_QRT   \n",
       "448  AMT_REQ_CREDIT_BUREAU_YEAR   \n",
       "\n",
       "                                         Description_x  Unnamed: 0  \\\n",
       "0           Unique identifier for the loan application           1   \n",
       "1                                                  NaN           2   \n",
       "2                       Type of loan cash or revolving           5   \n",
       "3                              Gender of the applicant           6   \n",
       "4                     Flag if the applicant owns a car           7   \n",
       "..                                                 ...         ...   \n",
       "444  Number of enquiries to Credit Bureau about the...         120   \n",
       "445  Number of enquiries to Credit Bureau about the...         121   \n",
       "446  Number of enquiries to Credit Bureau about the...         122   \n",
       "447  Number of enquiries to Credit Bureau about the...         123   \n",
       "448  Number of enquiries to Credit Bureau about the...         124   \n",
       "\n",
       "                        Table  \\\n",
       "0    application_{train|test}   \n",
       "1    application_{train|test}   \n",
       "2    application_{train|test}   \n",
       "3    application_{train|test}   \n",
       "4    application_{train|test}   \n",
       "..                        ...   \n",
       "444          application_test   \n",
       "445          application_test   \n",
       "446          application_test   \n",
       "447          application_test   \n",
       "448          application_test   \n",
       "\n",
       "                                         Description_y Special Unnamed: 5  \\\n",
       "0                             ID of loan in our sample     NaN        NaN   \n",
       "1    Target variable (1 - client with payment diffi...     NaN        NaN   \n",
       "2          Identification if loan is cash or revolving     NaN        NaN   \n",
       "3                                 Gender of the client     NaN        NaN   \n",
       "4                        Flag if the client owns a car     NaN        NaN   \n",
       "..                                                 ...     ...        ...   \n",
       "444  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "445  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "446  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "447  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "448  Number of enquiries to Credit Bureau about the...     NaN        NaN   \n",
       "\n",
       "    Unnamed: 6  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "..         ...  \n",
       "444        NaN  \n",
       "445        NaN  \n",
       "446        NaN  \n",
       "447        NaN  \n",
       "448        NaN  \n",
       "\n",
       "[449 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_des = {}\n",
    "row = []\n",
    "des = []\n",
    "for key,value in table_des.items():\n",
    "    row.append(key)\n",
    "    des.append(value)\n",
    "gpt_des['Row'] = row\n",
    "gpt_des['Description'] = des\n",
    "gpt_des = pd.DataFrame(gpt_des)\n",
    "result = pd.merge(gpt_des, official_des, on='Row',how='right')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result[['Row', 'Description_x', 'Description_y']]\n",
    "result.columns = ['Row', 'GPT Description', 'Official Description']\n",
    "result.to_csv('home_credit_default_application_test_gpt_des.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_create_database_statement(folder_path):\n",
    "    # 获取指定文件夹下的所有文件\n",
    "    files = os.listdir(folder_path)\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.sql'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # 获取数据库名\n",
    "            database_name = os.path.splitext(file_name)[0]\n",
    "            # 创建新文件来保存修改后的内容\n",
    "            temp_file_path = file_path + '.temp'\n",
    "            with open(file_path, 'r', encoding='utf-8') as original_file, \\\n",
    "                 open(temp_file_path, 'w', encoding='utf-8') as temp_file:\n",
    "                first_line = original_file.readline()\n",
    "                temp_file.write(first_line)\n",
    "                # 在第一行添加 CREATE DATABASE 语句\n",
    "                temp_file.write(f\"USE `{database_name}`;\\n\")\n",
    "                # 复制原文件内容到临时文件\n",
    "                temp_file.write(original_file.read())\n",
    "            # 替换原文件\n",
    "            os.remove(file_path)\n",
    "            os.rename(temp_file_path, file_path)\n",
    "\n",
    "# 指定文件夹路径\n",
    "folder_path = './new_CTU_dbs/'\n",
    "add_create_database_statement(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
