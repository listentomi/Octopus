# Installation
```
pip install -r requirements.txt
```
# Usage/Examples
## Step 1: Generate predicted SQLs using LLMs
```
python model_generate_sql_test.py --max_database_info_lines <max_lines> --database_info_path <db_info_path> --input_path <input_file> --output_sql_path <output_file> --model_choice <model_choice>
```
Replace <max_lines>, <db_info_path>, <input_file>, <output_file>, and <model_choice> with your actual values. 

Parameters explanation:

1. --max_database_info_lines <max_lines>: This argument sets the maximum number of lines to read from the database info file. Replace <max_lines> with the actual maximum number of lines you want to read. The default value is 1000.

2. --database_info_path <db_info_path>: This argument specifies the path to the database info file. Replace <db_info_path> with the actual path to your database info file. The default path is .\\test\\test_databases_create_info.

3. --input_path <input_file>: This argument specifies the path to the input file. Replace <input_file> with the actual path to your input file. The default path is .\\test\\test.json.

4. --output_sql_path <output_file>: This argument specifies the path to the output SQL file. Replace <output_file> with the actual path to your output SQL file. The default path is .\\test\\test_generated_sqls_wizardLM_13b.json.

5. --model_choice <model_choice>: This argument specifies the choice of the model. Replace <model_choice> with your actual model choice. The default model choice is WizardLM/WizardLM-13B-V1.2.


## Step 2: Process raw data formats for evaluating execution accuracy
Extract the database name and gold SQL from the raw data files and organize them into a format that the `evaluator.py` can read.
```
python extract_db_sql.py --input_file <input_json_file> --output_file <output_json_file>
```
## Step 3: Get execution accuracy
```
python evaluator.py --db_dir <database_directory> --compare_file_1 <gold_sql_filepath> --compare_file_2 <predict_sql_filepath> --output_path <result_output_file>
```
Replace `<database_directory>`, `<gold_sql_filepath>`, `<predict_sql_filepath>`, and `<result_output_file>` with your actual paths.

Parameters explanation:
1. --db_dir <database_directory>
    - Description: Specifies the directory where the database files are located. This is used by the evaluator to access the necessary databases for comparing the SQL queries.
    - Example: --db_dir /path/to/database_directory
2. --compare_file_1 <gold_sql_filepath>
    - Description: Indicates the file path to the first set of gold standard SQL queries. These are the expected correct queries that will be used as a reference for comparison.
    - Example: --compare_file_1 /path/to/gold_sql_file.sql
3. --compare_file_2 <predict_sql_filepath>
    - Description: Indicates the file path to the predicted SQL queries. These are the queries generated by a model or system that will be compared against the gold standard queries.
    - Example: --compare_file_2 /path/to/predict_sql_file.sql
4. --output_path <result_output_file>
    - Description: Specifies the file path where the results of the evaluation will be saved. This output file will contain the comparison results between the two sets of SQL queries.
    - Example: --output_path /path/to/result_output_file.json